(window.webpackJsonp=window.webpackJsonp||[]).push([[2956],{3299:function(t,a,s){"use strict";s.r(a);var r=s(19),e=Object(r.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"spark-api-sparkr"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#spark-api-sparkr"}},[t._v("#")]),t._v(" Spark API (SparkR)")]),t._v(" "),s("h2",{attrs:{id:"setup-spark-context"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#setup-spark-context"}},[t._v("#")]),t._v(" Setup Spark context")]),t._v(" "),s("h3",{attrs:{id:"setup-spark-context-in-r"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#setup-spark-context-in-r"}},[t._v("#")]),t._v(" Setup Spark context in R")]),t._v(" "),s("p",[t._v("To start working with Sparks distributed dataframes, you must connect your R program with an existing Spark Cluster.")]),t._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("library"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("SparkR"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsc "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" sparkR.init"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# connection to Spark context")]),t._v("\nsqlContext "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" sparkRSQL.init"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# connection to SQL context")]),t._v("\n\n")])])]),s("p",[s("a",{attrs:{href:"https://spark.apache.org/docs/1.6.0/sparkr.html#starting-up-from-rstudio",target:"_blank",rel:"noopener noreferrer"}},[t._v("Here are infos"),s("OutboundLink")],1),t._v(" how to connect your IDE to a Spark cluster.")]),t._v(" "),s("h3",{attrs:{id:"get-spark-cluster"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#get-spark-cluster"}},[t._v("#")]),t._v(" Get Spark Cluster")]),t._v(" "),s("p",[t._v("There is an "),s("a",{attrs:{href:"http://stackoverflow.com/documentation/apache-spark/833/introduction-to-apache-spark#t=201608091436462968765",target:"_blank",rel:"noopener noreferrer"}},[t._v("Apache Spark introduction topic"),s("OutboundLink")],1),t._v(" with install instructions. Basically, you can employ a Spark Cluster locally via java ("),s("a",{attrs:{href:"http://spark.apache.org/docs/latest/",target:"_blank",rel:"noopener noreferrer"}},[t._v("see instructions"),s("OutboundLink")],1),t._v(") or use (non-free) cloud applications (e.g. "),s("a",{attrs:{href:"https://azure.microsoft.com/en-us/services/hdinsight/apache-spark/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Microsoft Azure"),s("OutboundLink")],1),t._v(" "),s("a",{attrs:{href:"http://stackoverflow.com/documentation/azure/topics",target:"_blank",rel:"noopener noreferrer"}},[t._v(" [topic site]"),s("OutboundLink")],1),t._v(", "),s("a",{attrs:{href:"http://www.ibm.com/analytics/us/en/technology/spark/",target:"_blank",rel:"noopener noreferrer"}},[t._v("IBM"),s("OutboundLink")],1),t._v(").")]),t._v(" "),s("h2",{attrs:{id:"cache-data"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#cache-data"}},[t._v("#")]),t._v(" Cache data")]),t._v(" "),s("p",[t._v("What:")]),t._v(" "),s("p",[t._v("Caching can optimize computation in Spark. Caching stores data in memory and is a special case of persistence. "),s("a",{attrs:{href:"http://stackoverflow.com/a/28983767/3889242",target:"_blank",rel:"noopener noreferrer"}},[t._v("Here is explained"),s("OutboundLink")],1),t._v(" what happens when you cache an RDD in Spark.")]),t._v(" "),s("p",[t._v("Why:")]),t._v(" "),s("p",[t._v("Basically, caching saves an interim partial result - usually after transformations - of your original data. So, when you use the cached RDD, the already transformed data from memory is accessed without recomputing the earlier transformations.")]),t._v(" "),s("p",[t._v("How:")]),t._v(" "),s("p",[t._v("Here is an example how to quickly access large data "),s("strong",[t._v("(here 3 GB big csv)")]),t._v(" from in-memory storage when accessing it more then once:")]),t._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("library"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("SparkR"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# next line is needed for direct csv import:")]),t._v("\nSys.setenv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'SPARKR_SUBMIT_ARGS'")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('\'"--packages" "com.databricks:spark-csv_2.10:1.4.0" "sparkr-shell"\'')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsc "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" sparkR.init"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsqlContext "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" sparkRSQL.init"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# loading 3 GB big csv file:  ")]),t._v("\ntrain "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" read.df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sqlContext"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/train.csv"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" source "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"com.databricks.spark.csv"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inferSchema "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"true"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ncache"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsystem.time"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# output: time elapsed: 125 s. This action invokes the caching at this point.")]),t._v("\nsystem.time"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# output: time elapsed: 0.2 s (!!)")]),t._v("\n\n")])])]),s("h2",{attrs:{id:"create-rdds-resilient-distributed-datasets"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#create-rdds-resilient-distributed-datasets"}},[t._v("#")]),t._v(" Create RDDs (Resilient Distributed Datasets)")]),t._v(" "),s("h3",{attrs:{id:"from-dataframe"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#from-dataframe"}},[t._v("#")]),t._v(" From dataframe:")]),t._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("mtrdd "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" createDataFrame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sqlContext"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mtcars"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("h3",{attrs:{id:"from-csv"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#from-csv"}},[t._v("#")]),t._v(" From csv:")]),t._v(" "),s("p",[t._v("For csv's, you need to add the "),s("a",{attrs:{href:"https://github.com/databricks/spark-csv",target:"_blank",rel:"noopener noreferrer"}},[t._v("csv package"),s("OutboundLink")],1),t._v(" to the environment before initiating the Spark context:")]),t._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("Sys.setenv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'SPARKR_SUBMIT_ARGS'")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('\'"--packages" "com.databricks:spark-csv_2.10:1.4.0" "sparkr-shell"\'')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# context for csv import read csv -> ")]),t._v("\nsc "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" sparkR.init"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsqlContext "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" sparkRSQL.init"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("p",[t._v("Then, you can load the csv either by infering the data schema of the data in the columns:")]),t._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("train "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" read.df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sqlContext"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/train.csv"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" header"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"true"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" source "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"com.databricks.spark.csv"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inferSchema "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"true"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("p",[t._v("Or by specifying the data schema beforehand:")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v('\ncustomSchema <- structType(\n    structField("margin", "integer"),\n    structField("gross", "integer"),\n    structField("name", "string"))\n\n train <- read.df(sqlContext, "/train.csv", header= "true", source = "com.databricks.spark.csv", schema = customSchema)\n\n')])])]),s("h4",{attrs:{id:"remarks"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#remarks"}},[t._v("#")]),t._v(" Remarks")]),t._v(" "),s("p",[t._v("The "),s("code",[t._v("SparkR")]),t._v(" package let's you work with distributed data frames on top of a "),s("a",{attrs:{href:"http://spark.apache.org/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark cluster"),s("OutboundLink")],1),t._v(". These allow you to do operations like selection, filtering, aggregation on very large datasets.\n"),s("a",{attrs:{href:"https://spark.apache.org/docs/latest/sparkr.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("SparkR overview"),s("OutboundLink")],1),t._v(" "),s("a",{attrs:{href:"https://spark.apache.org/docs/1.5.1/api/R/",target:"_blank",rel:"noopener noreferrer"}},[t._v("SparkR package documentation"),s("OutboundLink")],1)])])}),[],!1,null,null,null);a.default=e.exports}}]);