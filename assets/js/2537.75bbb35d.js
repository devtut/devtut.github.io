(window.webpackJsonp=window.webpackJsonp||[]).push([[2537],{2882:function(t,a,s){"use strict";s.r(a);var e=s(19),n=Object(e.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"aggregate-functions"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#aggregate-functions"}},[t._v("#")]),t._v(" Aggregate Functions")]),t._v(" "),s("h2",{attrs:{id:"simple-statistics-min-max-avg"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#simple-statistics-min-max-avg"}},[t._v("#")]),t._v(" Simple statistics: min(), max(), avg()")]),t._v(" "),s("p",[t._v("In order to determine some simple statistics of a value in a column of a table, you can use an aggregate function.")]),t._v(" "),s("p",[t._v("If your "),s("code",[t._v("individuals")]),t._v(" table is:")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("Name")]),t._v(" "),s("th",[t._v("Age")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("Allie")]),t._v(" "),s("td",[t._v("17")])]),t._v(" "),s("tr",[s("td",[t._v("Amanda")]),t._v(" "),s("td",[t._v("14")])]),t._v(" "),s("tr",[s("td",[t._v("Alana")]),t._v(" "),s("td",[t._v("20")])])])]),t._v(" "),s("p",[t._v("You could write this statement to get the minimum, maximum and average value:")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("min")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("age"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("max")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("age"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("avg")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("age"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" individuals"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n")])])]),s("p",[t._v("Result:")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("min")]),t._v(" "),s("th",[t._v("max")]),t._v(" "),s("th",[t._v("avg")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("14")]),t._v(" "),s("td",[t._v("20")]),t._v(" "),s("td",[t._v("17")])])])]),t._v(" "),s("h2",{attrs:{id:"string-agg-expression-delimiter"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#string-agg-expression-delimiter"}},[t._v("#")]),t._v(" string_agg(expression, delimiter)")]),t._v(" "),s("p",[t._v("You can concatenate strings separated by delimiter using the "),s("code",[t._v("string_agg()")]),t._v(" function.")]),t._v(" "),s("p",[t._v("If your "),s("code",[t._v("individuals")]),t._v(" table is:")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("Name")]),t._v(" "),s("th",[t._v("Age")]),t._v(" "),s("th",[t._v("Country")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("Allie")]),t._v(" "),s("td",[t._v("15")]),t._v(" "),s("td",[t._v("USA")])]),t._v(" "),s("tr",[s("td",[t._v("Amanda")]),t._v(" "),s("td",[t._v("14")]),t._v(" "),s("td",[t._v("USA")])]),t._v(" "),s("tr",[s("td",[t._v("Alana")]),t._v(" "),s("td",[t._v("20")]),t._v(" "),s("td",[t._v("Russia")])])])]),t._v(" "),s("p",[t._v("You could write "),s("code",[t._v("SELECT ... GROUP BY")]),t._v(" statement to get names from each country:")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" string_agg"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("', '")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" country \n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" individuals \n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" country"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n")])])]),s("p",[t._v("Note that you need to use a "),s("code",[t._v("GROUP BY")]),t._v(" clause because "),s("code",[t._v("string_agg()")]),t._v(" is an aggregate function.")]),t._v(" "),s("p",[s("strong",[t._v("Result:")])]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("names")]),t._v(" "),s("th",[t._v("country")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("Allie, Amanda")]),t._v(" "),s("td",[t._v("USA")])]),t._v(" "),s("tr",[s("td",[t._v("Alana")]),t._v(" "),s("td",[t._v("Russia")])])])]),t._v(" "),s("p",[s("a",{attrs:{href:"https://www.postgresql.org/docs/devel/static/functions-aggregate.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("More PostgreSQL aggregate function described here"),s("OutboundLink")],1)]),t._v(" "),s("h2",{attrs:{id:"regr-slope-y-x-slope-of-the-least-squares-fit-linear-equation-determined-by-the-x-y-pairs"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#regr-slope-y-x-slope-of-the-least-squares-fit-linear-equation-determined-by-the-x-y-pairs"}},[t._v("#")]),t._v(" regr_slope(Y, X) : slope of the least-squares-fit linear equation determined by the (X, Y) pairs")]),t._v(" "),s("p",[t._v("To illustrate how to use regr_slope(Y,X), I applied it to a real world problem. In Java, if you don't clean up memory properly, the garbage can get stuck and fill up the memory. You dump statistics every hour about memory utilization of different classes and load it into a postgres database for analysis.")]),t._v(" "),s("p",[t._v("All memory leak candidates will have a trend of consuming more memory as more time passes. If you plot this trend, you would imagine a line going up and to the left:")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("\n   ^\n    |\ns   |  Legend:\ni   |  *  - data point\nz   |  -- - trend\ne   |\n(   |\nb   |                 *\ny   |                     --\nt   |                  --\ne   |             * --    *\ns   |           --\n)   |       *--      *\n    |     --    *\n    |  -- *\n   ---------------------------------------\x3e\n                      time\n\n")])])]),s("p",[t._v("Suppose you have a table containing heap dump histogram data (a mapping of classes to how much memory they consume):")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CREATE")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" heap_histogram "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- when the heap histogram was taken")]),t._v("\n    histwhen "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("timestamp")]),t._v(" without "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("time")]),t._v(" zone "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("NOT")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- the object type bytes are referring to")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- ex: java.util.String")]),t._v("\n    class "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("character")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("varying")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("NOT")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- the size in bytes used by the above class")]),t._v("\n    bytes "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("integer")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("NOT")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n")])])]),s("p",[t._v("To compute the slope for each class, we group by over the class. The HAVING clause > 0 ensures that we get only candidates with a positive slop (a line going up and to the left). We sort by the slope descending so that we get the classes with the largest rate of memory increase at the top.")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- epoch returns seconds")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" class"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" REGR_SLOPE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bytes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("extract"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epoch "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" histwhen"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" slope\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("heap_histogram\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" class\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("HAVING")]),t._v(" REGR_SLOPE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bytes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("extract"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epoch "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" histwhen"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ORDER")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" slope "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DESC")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n")])])]),s("p",[t._v("Output:")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("\n        class             |        slope         \n---------------------------+----------------------\n java.util.ArrayList       |     71.7993806279174\n java.util.HashMap         |     49.0324576155785\n java.lang.String          |     31.7770770326123\n joe.schmoe.BusinessObject |     23.2036817108056\n java.lang.ThreadLocal     |     20.9013528767851\n\n")])])]),s("p",[t._v("From the output we see that java.util.ArrayList's memory consumption is increasing the fastest at 71.799 bytes per second and is potentially part of the memory leak.")])])}),[],!1,null,null,null);a.default=n.exports}}]);