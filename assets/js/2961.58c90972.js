(window.webpackJsonp=window.webpackJsonp||[]).push([[2961],{3369:function(t,s,a){"use strict";a.r(s);var n=a(31),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"split-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#split-function"}},[t._v("#")]),t._v(" Split function")]),t._v(" "),a("h2",{attrs:{id:"using-split-in-the-split-apply-combine-paradigm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#using-split-in-the-split-apply-combine-paradigm"}},[t._v("#")]),t._v(" Using split in the split-apply-combine paradigm")]),t._v(" "),a("p",[t._v("A popular form of data analysis is "),a("a",{attrs:{href:"https://www.jstatsoft.org/article/view/v040i01/v40i01.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("split-apply-combine"),a("OutboundLink")],1),t._v(", in which you split your data into groups, apply some sort of processing on each group, and then combine the results.")]),t._v(" "),a("p",[t._v("Let's consider a data analysis where we want to obtain the two cars with the best miles per gallon (mpg) for each cylinder count (cyl) in the built-in mtcars dataset. First, we split the "),a("code",[t._v("mtcars")]),t._v(" data frame by the cylinder count:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("spl "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("cyl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# $`4`")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Datsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Merc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Merc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ...")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# $`6`")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Valiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ...")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# $`8`")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ...")]),t._v("\n\n")])])]),a("p",[t._v("This has returned a list of data frames, one for each cylinder count. As indicated by the output, we could obtain the relevant data frames with "),a("code",[t._v("spl$")]),t._v("4"),a("code",[t._v(", `spl$`6")]),t._v(", and "),a("code",[t._v("spl$")]),t._v("8`` (some might find it more visually appealing to use "),a("code",[t._v('spl$"4"')]),t._v(" or "),a("code",[t._v('spl[["4"]]')]),t._v(" instead).")]),t._v(" "),a("p",[t._v("Now, we can use "),a("code",[t._v("lapply")]),t._v(" to loop through this list, applying our function that extracts the cars with the best 2 mpg values from each of the list elements:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("best2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" lapply"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("spl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" tail"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("order"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("mpg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# $`4`")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#                 mpg cyl disp hp drat    wt  qsec vs am gear carb")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Fiat 128       32.4   4 78.7 66 4.08 2.200 19.47  1  1    4    1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Toyota Corolla 33.9   4 71.1 65 4.22 1.835 19.90  1  1    4    1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# $`6`")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#                 mpg cyl disp  hp drat    wt  qsec vs am gear carb")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Mazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# $`8`")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#                    mpg cyl disp  hp drat    wt  qsec vs am gear carb")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Pontiac Firebird  19.2   8  400 175 3.08 3.845 17.05  0  0    3    2")]),t._v("\n\n")])])]),a("p",[t._v("Finally, we can combine everything together using "),a("code",[t._v("rbind")]),t._v(". We want to call "),a("code",[t._v('rbind(best2[["4"]], best2[["6"]], best2[["8"]])')]),t._v(", but this would be tedious if we had a huge list. As a result, we use:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("do.call"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rbind"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" best2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.Fiat 128          32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.Toyota Corolla    33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6.Mazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6.Hornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 8.Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 8.Pontiac Firebird  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2")]),t._v("\n\n")])])]),a("p",[t._v("This returns the result of "),a("code",[t._v("rbind")]),t._v(" (argument 1, a function) with all the elements of "),a("code",[t._v("best2")]),t._v(" (argument 2, a list) passed as arguments.")]),t._v(" "),a("p",[t._v("With simple analyses like this one, it can be more compact (and possibly much less readable!) to do the whole split-apply-combine in a single line of code:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("do.call"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rbind"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lapply"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("cyl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" tail"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("order"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("mpg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("It is also worth noting that the "),a("code",[t._v("lapply(split(x,f), FUN)")]),t._v(" combination can be  alternatively framed using the "),a("code",[t._v("?by")]),t._v(" function:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("by"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("cyl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" tail"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("order"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("mpg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndo.call"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rbind"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" by"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("cyl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" tail"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("order"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("mpg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("h2",{attrs:{id:"basic-usage-of-split"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#basic-usage-of-split"}},[t._v("#")]),t._v(" Basic usage of split")]),t._v(" "),a("p",[a("code",[t._v("split")]),t._v(" allows to divide a vector or a data.frame into buckets with regards to a factor/group variables. This ventilation into buckets takes the form of a list, that can then be used to apply group-wise computation ("),a("code",[t._v("for")]),t._v(" loops or "),a("code",[t._v("lapply")]),t._v("/"),a("code",[t._v("sapply")]),t._v(").")]),t._v(" "),a("p",[t._v("First example shows the usage of "),a("code",[t._v("split")]),t._v(" on a vector:")]),t._v(" "),a("p",[t._v("Consider following vector of letters:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("testdata "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"e"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"o"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"r"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"g"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"a"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"y"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"w"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"q"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"i"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"s"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"b"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"v"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"x"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"h"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"u"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("Objective is to separate those letters into "),a("code",[t._v("voyels")]),t._v(" and "),a("code",[t._v("consonants")]),t._v(", ie split it accordingly to letter type.")]),t._v(" "),a("p",[t._v("Let's first create a grouping vector:")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("\nvowels <- c('a','e','i','o','u','y')\n letter_type <- ifelse(testdata %in% vowels, \"vowels\", \"consonants\") \n\n")])])]),a("p",[t._v("Note that "),a("code",[t._v("letter_type")]),t._v(" has the same length that our vector "),a("code",[t._v("testdata")]),t._v(".\nNow we can "),a("code",[t._v("split")]),t._v(" this test data in the two groups, "),a("code",[t._v("vowels")]),t._v(" and "),a("code",[t._v("consonants")]),t._v(" :")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("testdata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" letter_type"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#$consonants")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('#[1] "r" "g" "w" "q" "s" "b" "v" "x" "h"')]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#$vowels")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('#[1] "e" "o" "a" "y" "i" "u"')]),t._v("\n\n")])])]),a("p",[t._v("Hence, the result is a list which names are coming from our grouping vector/factor "),a("code",[t._v("letter_type")]),t._v(".")]),t._v(" "),a("p",[a("code",[t._v("split")]),t._v(" has also a method to deal with data.frames.")]),t._v(" "),a("p",[t._v("Consider for instance "),a("code",[t._v("iris")]),t._v(" data:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("iris"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("By using "),a("code",[t._v("split")]),t._v(", one can create a list containing one data.frame per iris specie (variable: Species):")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" liris "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("iris"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" iris"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("Species"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" names"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("liris"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"setosa"')]),t._v("     "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"versicolor"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"virginica"')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" head"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("liris"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("setosa"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.1")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.5")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.4")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),t._v("  setosa\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.9")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.0")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.4")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),t._v("  setosa\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.7")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.2")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.3")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),t._v("  setosa\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.6")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.1")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.5")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),t._v("  setosa\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.0")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.6")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.4")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),t._v("  setosa\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.4")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.9")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.7")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4")]),t._v("  setosa\n\n")])])]),a("p",[t._v("(contains only data for setosa group).")]),t._v(" "),a("p",[t._v("One example operation would be to compute correlation matrix per iris specie; one would then use "),a("code",[t._v("lapply")]),t._v(":")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lcor "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" lapply"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("liris"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FUN"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" cor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("setosa\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0000000")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7425467")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2671758")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2780984")]),t._v("\nSepal.Width     "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7425467")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0000000")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1777000")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2327520")]),t._v("\nPetal.Length    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2671758")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1777000")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0000000")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.3316300")]),t._v("\nPetal.Width     "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2780984")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2327520")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.3316300")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0000000")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("versicolor\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0000000")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5259107")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7540490")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5464611")]),t._v("\nSepal.Width     "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5259107")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0000000")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5605221")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.6639987")]),t._v("\nPetal.Length    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7540490")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5605221")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0000000")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7866681")]),t._v("\nPetal.Width     "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5464611")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.6639987")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7866681")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0000000")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("virginica\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0000000")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4572278")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8642247")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2811077")]),t._v("\nSepal.Width     "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4572278")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0000000")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4010446")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5377280")]),t._v("\nPetal.Length    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8642247")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4010446")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0000000")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.3221082")]),t._v("\nPetal.Width     "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2811077")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5377280")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.3221082")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0000000")]),t._v("\n\n")])])]),a("p",[t._v("Then we can retrieve per group the best pair of correlated variables:\n(correlation matrix is reshaped/melted, diagonal is filtered out and selecting best record is performed)")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" library"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("topcor "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" lapply"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lcor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FUN"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cormat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n   correlations "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" melt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cormat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("variable_name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v('"correlatio'),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" \n   filtered "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" correlations"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("correlations"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("X1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" correlations"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("X2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n   filtered"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("which.max"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filtered"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("correlation"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    \n\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("setosa\n           X1           X2     correlation\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(" Sepal.Width Sepal.Length       "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7425467")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("versicolor\n            X1           X2     correlation\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),t._v(" Petal.Width Petal.Length       "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7866681")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("virginica\n            X1           X2     correlation\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v(" Petal.Length Sepal.Length       "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8642247")]),t._v("\n\n")])])]),a("p",[t._v("Note that one computations are performed on such groupwise level, one may be interested in stacking the results, which can be done with:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" do.call"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"rbind"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" topcor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n                     X1           X2     correlation\nsetosa      Sepal.Width Sepal.Length       "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7425467")]),t._v("\nversicolor  Petal.Width Petal.Length       "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7866681")]),t._v("\nvirginica  Petal.Length Sepal.Length       "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8642247")]),t._v("\n\n")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);