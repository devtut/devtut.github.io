(window.webpackJsonp=window.webpackJsonp||[]).push([[2928],{3336:function(a,t,s){"use strict";s.r(t);var e=s(31),n=Object(e.a)({},(function(){var a=this,t=a.$createElement,s=a._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[s("h1",{attrs:{id:"pivot-and-unpivot-with-data-table"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pivot-and-unpivot-with-data-table"}},[a._v("#")]),a._v(" Pivot and unpivot with data.table")]),a._v(" "),s("h2",{attrs:{id:"pivot-and-unpivot-tabular-data-with-data-table-i"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pivot-and-unpivot-tabular-data-with-data-table-i"}},[a._v("#")]),a._v(" Pivot and unpivot tabular data with data.table - I")]),a._v(" "),s("p",[a._v("Convert from wide form to long form")]),a._v(" "),s("p",[a._v("Load "),s("code",[a._v("data USArrests")]),a._v(" from "),s("code",[a._v("datasets")]),a._v(".")]),a._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[a._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"USArrests"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\nhead"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("USArrests"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n           Murder Assault UrbanPop Rape\nAlabama      "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("13.2")]),a._v("     "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("236")]),a._v("       "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("58")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("21.2")]),a._v("\nAlaska       "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("10.0")]),a._v("     "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("263")]),a._v("       "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("48")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("44.5")]),a._v("\nArizona       "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("8.1")]),a._v("     "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("294")]),a._v("       "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("80")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("31.0")]),a._v("\nArkansas      "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("8.8")]),a._v("     "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("190")]),a._v("       "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("50")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("19.5")]),a._v("\nCalifornia    "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("9.0")]),a._v("     "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("276")]),a._v("       "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("91")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("40.6")]),a._v("\nColorado      "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("7.9")]),a._v("     "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("204")]),a._v("       "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("78")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("38.7")]),a._v("\n\n")])])]),s("p",[a._v("Use "),s("code",[a._v("?USArrests")]),a._v(" to find out more. First, convert to "),s("code",[a._v("data.table")]),a._v(". The names of states are row names in the original "),s("code",[a._v("data.frame")]),a._v(".")]),a._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[a._v("library"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("data.table"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\nDT "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<-")]),a._v(" as.data.table"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("USArrests"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" keep.rownames"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("TRUE")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n")])])]),s("p",[a._v("This is data in the wide form. It has a column for each variable. The data can also be stored in long form without loss of information. The long form has one column that stores the variable names. Then, it has another column for the variable values. The long form of "),s("code",[a._v("USArrests")]),a._v(" looks like so.")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("\n           State    Crime  Rate\n  1:       Alabama   Murder  13.2\n  2:        Alaska   Murder  10.0\n  3:       Arizona   Murder   8.1\n  4:      Arkansas   Murder   8.8\n  5:    California   Murder   9.0\n ---                             \n196:      Virginia     Rape  20.7\n197:    Washington     Rape  26.2\n198: West Virginia     Rape   9.3\n199:     Wisconsin     Rape  10.8\n200:       Wyoming     Rape  15.6\n\n")])])]),s("p",[a._v("We use the "),s("code",[a._v("melt")]),a._v(" function to switch from wide form to long form.")]),a._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[a._v("DTm "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<-")]),a._v(" melt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("DT"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\nnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("DTm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<-")]),a._v(" c"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"State"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"Crime"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"Rate"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n")])])]),s("p",[a._v("By default, "),s("code",[a._v("melt")]),a._v(" treats all columns with numeric data as variables with values. In "),s("code",[a._v("USArrests")]),a._v(", the variable "),s("code",[a._v("UrbanPop")]),a._v(" represents the percentage urban population of a state. It is different from the other variabes, "),s("code",[a._v("Murder")]),a._v(", "),s("code",[a._v("Assault")]),a._v(" and "),s("code",[a._v("Rape")]),a._v(", which are violent crimes reported per 100,000 people. Suppose we want to retain "),s("code",[a._v("UrbanPop")]),a._v(" column. We achieve this by setting "),s("code",[a._v("id.vars")]),a._v(" as follows.")]),a._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[a._v("DTmu "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<-")]),a._v(" melt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("DT"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" id.vars"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("c"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"rn"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"UrbanPop"')]),a._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" \n             variable.name"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'Crime'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" value.name "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"Rate"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\nnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("DTmu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<-")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"State"')]),a._v("\n\n")])])]),s("p",[a._v("Note that we have specified the names of the column containing category names (Murder, Assault, etc.) with "),s("code",[a._v("variable.name")]),a._v(" and the column containing the values with "),s("code",[a._v("value.name")]),a._v(". Our data looks like so.")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("\n            State UrbanPop  Crime Rate\n  1:       Alabama       58 Murder 13.2\n  2:        Alaska       48 Murder 10.0\n  3:       Arizona       80 Murder  8.1\n  4:      Arkansas       50 Murder  8.8\n  5:    California       91 Murder  9.0\n\n")])])]),s("p",[a._v("Generating summaries with with split-apply-combine style approach is a breeze. For example, to summarize violent crimes by state?")]),a._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[a._v("DTmu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" ."),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("ViolentCrime "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" sum"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("Rate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" by"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("State"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\n\n")])])]),s("p",[a._v("This gives:")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("\n       State ViolentCrime\n1:    Alabama        270.4\n2:     Alaska        317.5\n3:    Arizona        333.1\n4:   Arkansas        218.3\n5: California        325.6\n6:   Colorado        250.6\n\n")])])]),s("h2",{attrs:{id:"pivot-and-unpivot-tabular-data-with-data-table-ii"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pivot-and-unpivot-tabular-data-with-data-table-ii"}},[a._v("#")]),a._v(" Pivot and unpivot tabular data with data.table - II")]),a._v(" "),s("p",[a._v("Convert from long form to wide form")]),a._v(" "),s("p",[a._v("To recover data from the previous example, use "),s("code",[a._v("dcast")]),a._v(" like so.")]),a._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[a._v("DTc "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<-")]),a._v(" dcast"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("DTmu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" State "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("+")]),a._v(" UrbanPop "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("~")]),a._v(" Crime"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n")])])]),s("p",[a._v("This gives the data in the original wide form.")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("\n            State UrbanPop Murder Assault Rape\n 1:        Alabama       58   13.2     236 21.2\n 2:         Alaska       48   10.0     263 44.5\n 3:        Arizona       80    8.1     294 31.0\n 4:       Arkansas       50    8.8     190 19.5\n 5:     California       91    9.0     276 40.6\n\n")])])]),s("p",[a._v("Here, the formula notation is used to specify the columns that form a unique record identifier (LHS) and the column containing category labels for new column names (RHS). Which column to use for the numeric values? By default, "),s("code",[a._v("dcast")]),a._v(" uses the first column with numerical values left over when from the formula specification. To make explicit, use the parameter "),s("code",[a._v("value.var")]),a._v(" with column name.")]),a._v(" "),s("p",[a._v("When the operation produces a list of values in each cell, "),s("code",[a._v("dcast")]),a._v(" provides a "),s("code",[a._v("fun.aggregate")]),a._v(" method to handle the situation. Say I am interested in states with similar urban population when investigating crime rates. I add a column "),s("code",[a._v("Decile")]),a._v(" with computed information.")]),a._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[a._v("DTmu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" Decile "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" cut"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("UrbanPop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" quantile"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("UrbanPop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" probs "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" seq"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" by"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\nlevels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("DTmu"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("$")]),a._v("Decile"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<-")]),a._v(" paste0"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"D"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n")])])]),s("p",[a._v("Now, casting "),s("code",[a._v("Decile ~ Crime")]),a._v(" produces multiple values per cell. I can use "),s("code",[a._v("fun.aggregate")]),a._v(" to determine how these are handled. Both text and numerical values can be handle this way.")]),a._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[a._v("dcast"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("DTmu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" Decile "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("~")]),a._v(" Crime"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" value.var"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"Rate"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" fun.aggregate"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("sum"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n")])])]),s("p",[a._v("This gives:")]),a._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[a._v("dcast"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("DTmu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" Decile "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("~")]),a._v(" Crime"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" value.var"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"Rate"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" fun.aggregate"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n")])])]),s("p",[a._v("This gives:")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("\n            State UrbanPop  Crime Rate Decile\n  1:       Alabama       58 Murder 13.2     4D\n  2:        Alaska       48 Murder 10.0     2D\n  3:       Arizona       80 Murder  8.1     8D\n  4:      Arkansas       50 Murder  8.8     2D\n  5:    California       91 Murder  9.0    10D\n\n")])])]),s("p",[a._v("There are multiple states in each decile of the urban population. Use "),s("code",[a._v("fun.aggregate")]),a._v(" to specify how these should be handled.")]),a._v(" "),s("div",{staticClass:"language-r extra-class"},[s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[a._v("dcast"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("DTmu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" Decile "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("~")]),a._v(" Crime"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" value.var"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"Rate"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" fun.aggregate"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("sum"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n")])])]),s("p",[a._v("This sums over the data for like states, giving the following.")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("\n   Decile Murder Assault  Rape\n 1:     1D   39.4     808  62.6\n 2:     2D   35.3     815  94.3\n 3:     3D   22.6     451  67.7\n 4:     4D   54.9     898 106.0\n 5:     5D   42.4     758 107.6 \n\n")])])]),s("h4",{attrs:{id:"syntax"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#syntax"}},[a._v("#")]),a._v(" Syntax")]),a._v(" "),s("ul",[s("li",[a._v("Melt with "),s("code",[a._v('melt(DT, id.vars=c(..), variable.name="CategoryLabel", value.name="Value")')])]),a._v(" "),s("li",[a._v("Cast with "),s("code",[a._v('dcast(DT, LHS ~ RHS, value.var="Value", fun.aggregate=sum)')])])]),a._v(" "),s("h4",{attrs:{id:"parameters"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#parameters"}},[a._v("#")]),a._v(" Parameters")]),a._v(" "),s("table",[s("thead",[s("tr",[s("th",[a._v("Parameter")]),a._v(" "),s("th",[a._v("Details")])])]),a._v(" "),s("tbody",[s("tr",[s("td",[a._v("id.vars")]),a._v(" "),s("td",[a._v("tell "),s("code",[a._v("melt")]),a._v(" which columns to retain")])]),a._v(" "),s("tr",[s("td",[a._v("variable.name")]),a._v(" "),s("td",[a._v("tell "),s("code",[a._v("melt")]),a._v(" what to call the column with category labels")])]),a._v(" "),s("tr",[s("td",[a._v("value.name")]),a._v(" "),s("td",[a._v("tell "),s("code",[a._v("melt")]),a._v(" what to call the column that has values associated with category labels")])]),a._v(" "),s("tr",[s("td",[a._v("value.var")]),a._v(" "),s("td",[a._v("tell "),s("code",[a._v("dcast")]),a._v(" where to find the values to cast in columns")])]),a._v(" "),s("tr",[s("td",[a._v("formula")]),a._v(" "),s("td",[a._v("tell "),s("code",[a._v("dcast")]),a._v(" which columns to retain to form a unique record identifier (LHS) and which one holds the category labels (RHS)")])]),a._v(" "),s("tr",[s("td",[a._v("fun.aggregate")]),a._v(" "),s("td",[a._v("specify the function to use when the casting operation generates a list of values in each cell")])])])]),a._v(" "),s("h4",{attrs:{id:"remarks"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#remarks"}},[a._v("#")]),a._v(" Remarks")]),a._v(" "),s("p",[a._v("Much of what goes into conditioning data to build models or visualizations can be accomplished with "),s("code",[a._v("data.table")]),a._v(". As compare to other options, "),s("code",[a._v("data.table")]),a._v(" offers advantages of speed and flexibility.")])])}),[],!1,null,null,null);t.default=n.exports}}]);