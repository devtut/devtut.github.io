(window.webpackJsonp=window.webpackJsonp||[]).push([[2919],{3326:function(t,s,a){"use strict";a.r(s);var n=a(31),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"natural-language-processing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#natural-language-processing"}},[t._v("#")]),t._v(" Natural language processing")]),t._v(" "),a("p",[t._v("Natural language processing (NLP) is the field of computer sciences focused on retrieving information from textual input generated by human beings.")]),t._v(" "),a("h2",{attrs:{id:"create-a-term-frequency-matrix"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#create-a-term-frequency-matrix"}},[t._v("#")]),t._v(" Create a term frequency matrix")]),t._v(" "),a("p",[t._v("The simplest approach to the problem (and the most commonly used so far) is to split sentences into "),a("strong",[t._v("tokens")]),t._v(". Simplifying, "),a("strong",[t._v("words")]),t._v(" have abstract and subjective meanings to the people using and receiving them, "),a("strong",[t._v("tokens")]),t._v(" have an objective interpretation: an ordered sequence of characters (or bytes). Once sentences are split, the order of the token is disregarded. This approach to the problem in known as "),a("strong",[t._v("bag of words")]),t._v(" model.")]),t._v(" "),a("p",[t._v("A "),a("strong",[t._v("term frequency")]),t._v(" is a dictionary, in which to each token is assigned a "),a("strong",[t._v("weight")]),t._v(". In the first example, we construct a term frequency matrix from a corpus "),a("strong",[t._v("corpus")]),t._v(" (a collection of "),a("strong",[t._v("documents")]),t._v(") with the R package "),a("code",[t._v("tm")]),t._v(".")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("require"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndoc1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"drugs hospitals doctors"')]),t._v("\ndoc2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"smog pollution environment"')]),t._v("\ndoc3 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"doctors hospitals healthcare"')]),t._v("\ndoc4 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pollution environment water"')]),t._v("\ncorpus "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("doc1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" doc2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" doc3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" doc4"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntm_corpus "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" Corpus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("VectorSource"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("corpus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("In this example, we created a corpus of class "),a("code",[t._v("Corpus")]),t._v(" defined by the package "),a("code",[t._v("tm")]),t._v(" with two functions "),a("code",[t._v("Corpus")]),t._v(" and "),a("code",[t._v("VectorSource")]),t._v(", which returns a "),a("code",[t._v("VectorSource")]),t._v(" object from a character vector. The object "),a("code",[t._v("tm_corpus")]),t._v(" is a list our documents with additional (and optional) metadata to describe each document.")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("str"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tm_corpus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nList of "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("\n "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("List of "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n  .."),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v(" content"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" chr "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"drugs hospitals doctors"')]),t._v("\n  .."),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v(" meta   "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("List of "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),t._v("\n  .. .."),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v(" author       "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" chr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n  .. .."),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v(" datetimestamp"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" POSIXlt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" format"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"2017-06-03 00:31:34"')]),t._v("\n  .. .."),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v(" description  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" chr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n  .. .."),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v(" heading      "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" chr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n  .. .."),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v(" id           "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" chr "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"1"')]),t._v("\n  .. .."),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v(" language     "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" chr "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"en"')]),t._v("\n  .. .."),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v(" origin       "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" chr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n  .. .."),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" attr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"class"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" chr "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextDocumentMeta"')]),t._v("\n  .."),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" attr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"class"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" chr "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"PlainTextDocument"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextDocument"')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("truncated"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n")])])]),a("p",[t._v("Once we have a "),a("code",[t._v("Corpus")]),t._v(", we can proceed to preprocess the tokens contained in the "),a("code",[t._v("Corpus")]),t._v(" to improve the quality of the final output (the term frequency matrix). To do this we use the "),a("code",[t._v("tm")]),t._v(" function "),a("code",[t._v("tm_map")]),t._v(", which similarly to the "),a("code",[t._v("apply")]),t._v(" family of functions, transform the documents in the corpus by applying a function to each document.")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("tm_corpus "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" tm_map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tm_corpus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tolower"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntm_corpus "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" tm_map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tm_corpus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" removeWords"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stopwords"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"english"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntm_corpus "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" tm_map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tm_corpus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" removeNumbers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntm_corpus "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" tm_map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tm_corpus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" PlainTextDocument"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntm_corpus "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" tm_map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tm_corpus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stemDocument"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" language"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"english"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntm_corpus "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" tm_map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tm_corpus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stripWhitespace"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntm_corpus "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" tm_map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tm_corpus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" PlainTextDocument"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("Following these transformations, we finally create the term frequency matrix with")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("tdm "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" TermDocumentMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tm_corpus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("which gives a")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("TermDocumentMatrix "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("terms"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" documents"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\nNon"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("sparse entries"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\nSparsity           "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("62")]),t._v("%\nMaximal term length"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),t._v("\nWeighting          "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" term frequency "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("that we can view by transforming it to a matrix")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("as.matrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tdm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n           Docs\nTerms       character"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" character"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" character"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" character"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  doctor               "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n  drug                 "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n  environ              "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n  healthcar            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n  hospit               "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n  pollut               "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n  smog                 "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n  water                "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n\n")])])]),a("p",[t._v("Each row represents the frequency of each token - that as you noticed have been stemmed (e.g. "),a("code",[t._v("environment")]),t._v(" to "),a("code",[t._v("environ")]),t._v(") - in each document (4 documents, 4 columns).")]),t._v(" "),a("p",[t._v("In the previous lines, we have weighted each pair token/document with the absolute frequency (i.e. the number of instances of the token that appear in the document).")])])}),[],!1,null,null,null);s.default=e.exports}}]);