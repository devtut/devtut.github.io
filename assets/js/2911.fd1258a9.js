(window.webpackJsonp=window.webpackJsonp||[]).push([[2911],{3319:function(t,s,a){"use strict";a.r(s);var e=a(31),n=Object(e.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"linear-models-regression"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#linear-models-regression"}},[t._v("#")]),t._v(" Linear Models (Regression)")]),t._v(" "),a("h2",{attrs:{id:"linear-regression-on-the-mtcars-dataset"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#linear-regression-on-the-mtcars-dataset"}},[t._v("#")]),t._v(" Linear regression on the mtcars dataset")]),t._v(" "),a("p",[t._v("The built-in mtcars "),a("a",{attrs:{href:"http://stackoverflow.com/documentation/r/438/data-frames#t=201607172216448376641",target:"_blank",rel:"noopener noreferrer"}},[t._v("data frame"),a("OutboundLink")],1),t._v(" contains information about 32 cars, including their weight, fuel efficiency (in miles-per-gallon), speed, etc. (To find out more about the dataset, use "),a("code",[t._v("help(mtcars)")]),t._v(").")]),t._v(" "),a("p",[t._v("If we are interested in the relationship between fuel efficiency ("),a("code",[t._v("mpg")]),t._v(") and weight ("),a("code",[t._v("wt")]),t._v(") we may start plotting those variables with:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("plot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mpg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" wt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" col"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("The plots shows a (linear) relationship!. Then if we want to perform linear regression to determine the coefficients of a linear model, we would use the "),a("code",[t._v("lm")]),t._v(" function:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("fit "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" lm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mpg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" wt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("The "),a("code",[t._v("~")]),t._v(' here means "explained by", so the formula '),a("code",[t._v("mpg ~ wt")]),t._v(" means we are predicting mpg as explained by wt. The most helpful way to view the output is with:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("Which gives the output:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("Call"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\nlm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("formula "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mpg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" wt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nResiduals"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n    Min      "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("Q  Median      "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("Q     Max \n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.5432")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.3647")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1252")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.4096")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("6.8727")]),t._v(" \n\nCoefficients"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n            Estimate Std. Error t value Pr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Intercept"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("37.2851")]),t._v("     "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.8776")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("19.858")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2e-16")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\nwt           "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.3445")]),t._v("     "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5591")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9.559")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.29e-10")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("\nSignif. codes"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" ‘"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("’ "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.001")]),t._v(" ‘"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("’ "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),t._v(" ‘"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("’ "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.05")]),t._v(" ‘.’ "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),t._v(" ‘ ’ "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n\nResidual standard error"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.046")]),t._v(" on "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v(" degrees of freedom\nMultiple R"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("squared"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7528")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("    Adjusted R"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("squared"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7446")]),t._v(" \nF"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("statistic"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("91.38")]),t._v(" on "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" and "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v(" DF"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  p"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("value"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.294e-10")]),t._v("\n\n")])])]),a("p",[t._v("This provides information about:")]),t._v(" "),a("ul",[a("li",[t._v("the estimated slope of each coefficient ("),a("code",[t._v("wt")]),t._v(" and the y-intercept), which suggests the best-fit prediction of mpg is "),a("code",[t._v("37.2851 + (-5.3445) * wt")])]),t._v(" "),a("li",[t._v("The p-value of each coefficient, which suggests that the intercept and weight are probably not due to chance")]),t._v(" "),a("li",[t._v("Overall estimates of fit such as R^2 and adjusted R^2, which show how much of the variation in "),a("code",[t._v("mpg")]),t._v(" is explained by the model")])]),t._v(" "),a("p",[t._v("We could add a line to our first plot to show the predicted "),a("code",[t._v("mpg")]),t._v(":")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("abline"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("col"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("lwd"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("It is also possible to add the equation to that plot. First, get the coefficients with "),a("code",[t._v("coef")]),t._v(". Then using "),a("code",[t._v("paste0")]),t._v(" we collapse the coefficients with appropriate variables and "),a("code",[t._v("+/-")]),t._v(", to built the equation. Finally, we add it to the plot using "),a("code",[t._v("mtext")]),t._v(":")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("bs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" round"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("coef"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \nlmlab "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" paste0"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"mpg = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n             ifelse"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sign"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('" + "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('" - "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" abs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('" wt "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmtext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lmlab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" line"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n\n")])])]),a("p",[t._v("The result is:\n"),a("a",{attrs:{href:"https://i.stack.imgur.com/5q9yW.png",target:"_blank",rel:"noopener noreferrer"}},[a("img",{attrs:{src:"https://i.stack.imgur.com/5q9yW.png",alt:"enter image description here"}}),a("OutboundLink")],1)]),t._v(" "),a("h2",{attrs:{id:"weighting"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#weighting"}},[t._v("#")]),t._v(" Weighting")]),t._v(" "),a("p",[t._v("Sometimes we want the model to give more weight to some data points or examples than others. This is possible by specifying the weight for the input data while learning the model. There are generally two kinds of scenarios where we might use non-uniform weights over the examples:")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("Analytic Weights: Reflect the different levels of precision of different observations. For example, if analyzing data where each observation is the average results from a geographic area, the analytic weight is proportional to the inverse of the estimated variance.  Useful when dealing with averages in data by providing a proportional weight given the number of observations. "),a("a",{attrs:{href:"http://surveyanalysis.org/wiki/Different_Types_of_Weights",target:"_blank",rel:"noopener noreferrer"}},[t._v("Source"),a("OutboundLink")],1)])]),t._v(" "),a("li",[a("p",[t._v("Sampling Weights (Inverse Probability Weights - IPW): a statistical technique for calculating statistics standardized to a population different from that in which the data was collected. Study designs with a disparate sampling population and population of target inference (target population) are common in application.  Useful when dealing with data that have missing values. "),a("a",{attrs:{href:"https://en.wikipedia.org/wiki/Inverse_probability_weighting",target:"_blank",rel:"noopener noreferrer"}},[t._v("Source"),a("OutboundLink")],1)])])]),t._v(" "),a("p",[t._v("The "),a("code",[t._v("lm()")]),t._v(" function does analytic weighting. For sampling weights the "),a("code",[t._v("survey")]),t._v(" package is used to build a survey design object and run "),a("code",[t._v("svyglm()")]),t._v(". By default, the "),a("code",[t._v("survey")]),t._v(" package uses sampling weights. (NOTE: "),a("code",[t._v("lm()")]),t._v(", and "),a("code",[t._v("svyglm()")]),t._v(" with family "),a("code",[t._v("gaussian()")]),t._v(" will all produce the same point estimates, because they both solve for the coefficients by minimizing the weighted least squares. They differ in how standard errors are calculated.)")]),t._v(" "),a("p",[a("strong",[t._v("Test Data")])]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" structure"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lexptot "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9.1595012302023")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9.86330744180814")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("8.92372556833205")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("8.58202430280175")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10.1133857229336")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" progvillm "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" sexhead "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" agehead "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("79L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("43L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("52L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("48L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("35L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.04273509979248")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.01139605045319")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.01139605045319")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.01139605045319")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.76305216550827")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" .Names "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lexptot"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"progvillm"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"sexhead"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"agehead"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"weight"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" class "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tbl_df"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tbl"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"data.frame"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" row.names "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("NA")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5L")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[a("strong",[t._v("Analytic Weights")])]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("lm.analytic "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" lm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lexptot "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" progvillm "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" sexhead "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" agehead"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                            data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsummary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lm.analytic"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[a("strong",[t._v("Output")])]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("Call"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\nlm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("formula "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lexptot "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" progvillm "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" sexhead "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" agehead"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nWeighted Residuals"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n         "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("          "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(" \n "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9.249e-02")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.823e-01")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.000e+00")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("6.762e-01")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.527e-16")]),t._v("\n\nCoefficients"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n             Estimate Std. Error t value Pr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Intercept"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10.016054")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.744293")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.742")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.110")]),t._v("\nprogvillm   "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.781204")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.344974")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.581")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.665")]),t._v("\nsexhead      "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.306742")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.040625")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.295")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.818")]),t._v("\nagehead     "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.005983")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.032024")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.187")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.882")]),t._v("\n\nResidual standard error"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8971")]),t._v(" on "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" degrees of freedom\nMultiple R"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("squared"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.467")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Adjusted R"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("squared"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.132")]),t._v(" \nF"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("statistic"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2921")]),t._v(" on "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v(" and "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" DF"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  p"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("value"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8386")]),t._v("\n\n")])])]),a("p",[a("strong",[t._v("Sampling Weights (IPW)")])]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("library"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("survey"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("X "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("nrow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("             "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Create unique id")]),t._v("\n        \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Build survey design object with unique id, ipw, and data.frame")]),t._v("\ndes1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" svydesign"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v("X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Run glm with survey design object")]),t._v("\nprog.lm "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" svyglm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lexptot "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" progvillm "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" sexhead "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" agehead"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" design"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("des1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[a("strong",[t._v("Output")])]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("Call"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\nsvyglm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("formula "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lexptot "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" progvillm "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" sexhead "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" agehead"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" design "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" des1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nSurvey design"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\nsvydesign"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v("X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nCoefficients"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n             Estimate Std. Error t value Pr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Intercept"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10.016054")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.183942")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("54.452")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0117")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\nprogvillm   "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.781204")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.640372")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.220")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4371")]),t._v("  \nsexhead      "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.306742")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.397089")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.772")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5813")]),t._v("  \nagehead     "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.005983")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.014747")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.406")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7546")]),t._v("  \n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("\nSignif. codes"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" ‘"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("’ "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.001")]),t._v(" ‘"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("’ "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),t._v(" ‘"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("’ "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.05")]),t._v(" ‘.’ "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),t._v(" ‘ ’ "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dispersion parameter "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" gaussian family taken to be "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2078647")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nNumber of Fisher Scoring iterations"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n\n")])])]),a("h2",{attrs:{id:"using-the-predict-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#using-the-predict-function"}},[t._v("#")]),t._v(" Using the 'predict' function")]),t._v(" "),a("p",[t._v("Once a model is built "),a("code",[t._v("predict")]),t._v(" is the main function to test with new data. Our example will use the "),a("code",[t._v("mtcars")]),t._v(" built-in dataset to regress miles per gallon against displacement:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("my_mdl "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" lm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mpg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" disp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmy_mdl\n\nCall"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\nlm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("formula "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mpg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" disp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nCoefficients"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Intercept"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("         disp  \n   "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("29.59985")]),t._v("     "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.04122")]),t._v("\n\n")])])]),a("p",[t._v("If I had a new data source with displacement I could see the estimated miles per gallon.")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("set.seed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1234")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nnewdata "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" sample"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("disp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nnewdata\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("258.0")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("71.1")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("75.7")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("145.0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("400.0")]),t._v("\n\nnewdf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" data.frame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("disp"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("newdata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npredict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("my_mdl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newdf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n       "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("        "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("        "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("        "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("        "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("18.96635")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("26.66946")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("26.47987")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("23.62366")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("13.11381")]),t._v("\n\n")])])]),a("p",[t._v("The most important part of the process is to create a new data frame with the same column names as the original data. In this case, the original data had a column labeled "),a("code",[t._v("disp")]),t._v(", I was sure to call the new data that same name.")]),t._v(" "),a("p",[a("strong",[t._v("Caution")])]),t._v(" "),a("p",[t._v("Let's look at a few common pitfalls:")]),t._v(" "),a("li",[t._v("\nnot using a data.frame in the new object:\n"),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("my_mdl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newdata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nError "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" eval"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predvars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" env"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" \n   numeric "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'envir'")]),t._v(" arg not of length one\n\n")])])])]),t._v(" "),a("li",[t._v("\nnot using same names in new data frame:\n"),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("newdf2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" data.frame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newdata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npredict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("my_mdl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newdf2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nError "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" eval"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("expr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" envir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" enclos"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" object "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'disp'")]),t._v(" not found\n\n")])])])]),t._v(" "),a("p",[a("strong",[t._v("Accuracy")])]),t._v(" "),a("p",[t._v("To check the accuracy of the prediction you will need the actual y values of the new data. In this example, "),a("code",[t._v("newdf")]),t._v(" will need a column for 'mpg' and 'disp'.")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("newdf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" data.frame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mpg"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("mpg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" disp"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("disp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     mpg  disp")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1  21.0 160.0")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2  21.0 160.0")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3  22.8 108.0")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4  21.4 258.0")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 5  18.7 360.0")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6  18.1 225.0")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 7  14.3 360.0")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 8  24.4 146.7")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 9  22.8 140.8")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 10 19.2 167.6")]),t._v("\n\np "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("my_mdl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newdf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#root mean square error")]),t._v("\nsqrt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" newdf"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("mpg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" na.rm"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("TRUE")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.325148")]),t._v("\n\n")])])]),a("h2",{attrs:{id:"checking-for-nonlinearity-with-polynomial-regression"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#checking-for-nonlinearity-with-polynomial-regression"}},[t._v("#")]),t._v(" Checking for nonlinearity with polynomial regression")]),t._v(" "),a("p",[t._v("Sometimes when working with linear regression we need to check for non-linearity in the data. One way to do this is to fit a polynomial model and check whether it fits the data better than a linear model. There are other reasons, such as theoretical, that indicate to fit a quadratic or higher order model because it is believed that the variables relationship is inherently polynomial in nature.")]),t._v(" "),a("p",[t._v("Let's fit a quadratic model for the "),a("code",[t._v("mtcars")]),t._v(" dataset. For a linear model see "),a("a",{attrs:{href:"http://stackoverflow.com/documentation/r/801/linear-models-regression/2738/linear-regression-on-the-mtcars-dataset",target:"_blank",rel:"noopener noreferrer"}},[t._v("Linear regression on the mtcars dataset"),a("OutboundLink")],1),t._v(".")]),t._v(" "),a("p",[t._v("First we make a scatter plot of the variables "),a("code",[t._v("mpg")]),t._v(" (Miles/gallon), "),a("code",[t._v("disp")]),t._v(" (Displacement (cu.in.)), and "),a("code",[t._v("wt")]),t._v(" (Weight (1000 lbs)). The relationship among "),a("code",[t._v("mpg")]),t._v(" and "),a("code",[t._v("disp")]),t._v(" appears non-linear.")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("plot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"mpg"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"disp"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"wt"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[a("a",{attrs:{href:"https://i.stack.imgur.com/rT8kS.png",target:"_blank",rel:"noopener noreferrer"}},[a("img",{attrs:{src:"https://i.stack.imgur.com/rT8kS.png",alt:"enter image description here"}}),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("A linear fit will show that "),a("code",[t._v("disp")]),t._v(" is not significant.")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("fit0 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mpg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" wt"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("disp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsummary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fit0"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Coefficients:")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#            Estimate Std. Error t value Pr(>|t|)    ")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#(Intercept) 34.96055    2.16454  16.151 4.91e-16 ***")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#wt          -3.35082    1.16413  -2.878  0.00743 ** ")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#disp        -0.01773    0.00919  -1.929  0.06362 .  ")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#---")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Residual standard error: 2.917 on 29 degrees of freedom")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Multiple R-squared:  0.7809,    Adjusted R-squared:  0.7658")]),t._v("\n\n")])])]),a("p",[t._v("Then, to get the result of a quadratic model, we added "),a("code",[t._v("I(disp^2)")]),t._v(". The new model appears better when looking at "),a("code",[t._v("R^2")]),t._v(" and all variables are significant.")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("fit1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mpg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" wt"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("disp"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("I"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("disp"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsummary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fit1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Coefficients:")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#              Estimate Std. Error t value Pr(>|t|)    ")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#(Intercept) 41.4019837  2.4266906  17.061  2.5e-16 ***")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#wt          -3.4179165  0.9545642  -3.581 0.001278 ** ")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#disp        -0.0823950  0.0182460  -4.516 0.000104 ***")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#I(disp^2)    0.0001277  0.0000328   3.892 0.000561 ***")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#---")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Residual standard error: 2.391 on 28 degrees of freedom")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Multiple R-squared:  0.8578,    Adjusted R-squared:  0.8426 ")]),t._v("\n\n")])])]),a("p",[t._v("As we have three variables, the fitted model is a surface represented by:")]),t._v(" "),a("p",[a("strong",[a("code",[t._v("mpg = 41.4020-3.4179*wt-0.0824*disp+0.0001277*disp^2")])])]),t._v(" "),a("p",[t._v("Another way to specify polynomial regression is using "),a("code",[t._v("poly")]),t._v(" with parameter "),a("code",[t._v("raw=TRUE")]),t._v(", otherwise "),a("strong",[t._v("orthogonal polynomials")]),t._v(" will be considered (see the "),a("code",[t._v("help(ploy)")]),t._v(" for more information). We get the same result using:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mpg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" wt"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("poly"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("disp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" raw"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("TRUE")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("Finally, what if we need to show a plot of the estimated surface? Well there are many options to make "),a("code",[t._v("3D")]),t._v(" plots in "),a("code",[t._v("R")]),t._v(". Here we use "),a("code",[t._v("Fit3d")]),t._v(" from "),a("code",[t._v("p3d")]),t._v("package.")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("library"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p3d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nInit3d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("family"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"serif"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cex "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nPlot3d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mpg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" disp"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("wt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nAxes3d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nFit3d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fit1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[a("a",{attrs:{href:"https://i.stack.imgur.com/Tuodn.png",target:"_blank",rel:"noopener noreferrer"}},[a("img",{attrs:{src:"https://i.stack.imgur.com/Tuodn.png",alt:"enter image description here"}}),a("OutboundLink")],1)]),t._v(" "),a("h2",{attrs:{id:"plotting-the-regression-base"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#plotting-the-regression-base"}},[t._v("#")]),t._v(" Plotting The Regression (base)")]),t._v(" "),a("p",[t._v("Continuing on the "),a("code",[t._v("mtcars")]),t._v(" example, here is a simple way to produce a plot of your linear regression that is potentially suitable for publication.")]),t._v(" "),a("p",[t._v("First fit the linear model and")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("fit "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" lm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mpg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" wt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("Then plot the two variables of interest and add  the regression line within the definition domain:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("plot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("wt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("mpg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("pch"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("18")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" xlab "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'wt'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("ylab "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mpg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlines"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("min"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("wt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("max"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("wt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\nas.numeric"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data.frame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("wt"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("min"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("wt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("max"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("wt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("Almost there! The last step is to add to the plot, the regression equation, the rsquare as well as the correlation coefficient. This is done using the "),a("code",[t._v("vector")]),t._v(" function:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("rp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vector"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'expression'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nrp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" substitute"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("expression"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("italic"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" MYOTHERVALUE3 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" MYOTHERVALUE4 "),a("span",{pre:!0,attrs:{class:"token percent-operator operator"}},[t._v("%*%")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n          list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MYOTHERVALUE3 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" format"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("coefficients"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" digits "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                        MYOTHERVALUE4 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" format"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("coefficients"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" digits "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nrp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" substitute"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("expression"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("italic"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("R"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" MYVALUE"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n             list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MYVALUE "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" format"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("adj.r.squared"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("dig"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nrp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" substitute"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("expression"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Pearson"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("R "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" MYOTHERVALUE2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n             list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MYOTHERVALUE2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" format"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("wt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("mtcars"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("$")]),t._v("mpg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" digits "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nlegend"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"topright"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" legend "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bty "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("Note that you can add any other parameter such as the RMSE by adapting the vector function. Imagine you want a legend with 10 elements. The vector definition would be the following:")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[t._v("rp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vector"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'expression'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("and you will need to defined "),a("code",[t._v("r[1]")]),t._v(".... to "),a("code",[t._v("r[10]")])]),t._v(" "),a("p",[t._v("Here is the output:")]),t._v(" "),a("p",[a("a",{attrs:{href:"http://i.stack.imgur.com/l3Ach.png",target:"_blank",rel:"noopener noreferrer"}},[a("img",{attrs:{src:"http://i.stack.imgur.com/l3Ach.png",alt:"enter image description here"}}),a("OutboundLink")],1)]),t._v(" "),a("h2",{attrs:{id:"quality-assessment"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#quality-assessment"}},[t._v("#")]),t._v(" Quality assessment")]),t._v(" "),a("p",[t._v("After building a regression model it is important to check the result and decide if the model is appropriate and works well with the data at hand. This can be done by examining the residuals plot as well as other diagnostic plots.")]),t._v(" "),a("div",{staticClass:"language-r extra-class"},[a("pre",{pre:!0,attrs:{class:"language-r"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# fit the model")]),t._v("\nfit "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<-")]),t._v(" lm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mpg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("~")]),t._v(" wt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mtcars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ")]),t._v("\npar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mfrow"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# plot model object")]),t._v("\nplot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" which "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[a("a",{attrs:{href:"https://i.stack.imgur.com/tHoK5.jpg",target:"_blank",rel:"noopener noreferrer"}},[a("img",{attrs:{src:"https://i.stack.imgur.com/tHoK5.jpg",alt:"enter image description here"}}),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("These plots check for two assumptions that were made while building the model:")]),t._v(" "),a("ol",[a("li",[t._v("That the expected value of the predicted variable (in this case "),a("code",[t._v("mpg")]),t._v(") is given by a linear combination of the predictors (in this case "),a("code",[t._v("wt")]),t._v("). We expect this estimate to be unbiased. So the residuals should be centered around the mean for all values of the predictors. In this case we see that the residuals tend to be positive at the ends and negative in the middle, suggesting a non-linear relationship between the variables.")]),t._v(" "),a("li",[t._v("That the actual predicted variable is normally distributed around its estimate. Thus, the residuals should be normally distributed. For normally distributed data, the points in a normal Q-Q plot should lie on or close to the diagonal. There is some amount of skew at the ends here.")])]),t._v(" "),a("h4",{attrs:{id:"syntax"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#syntax"}},[t._v("#")]),t._v(" Syntax")]),t._v(" "),a("ul",[a("li",[t._v('lm(formula, data, subset, weights, na.action, method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...)')])]),t._v(" "),a("h4",{attrs:{id:"parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",[t._v("Parameter")]),t._v(" "),a("th",[t._v("Meaning")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("formula")]),t._v(" "),a("td",[t._v("a formula in "),a("strong",[t._v("Wilkinson-Rogers")]),t._v(" notation; "),a("code",[t._v("response ~ ...")]),t._v(" where  "),a("code",[t._v("...")]),t._v(" contains terms corresponding to variables in the environment or in the data frame specified by the "),a("code",[t._v("data")]),t._v(" argument")])]),t._v(" "),a("tr",[a("td",[t._v("data")]),t._v(" "),a("td",[t._v("data frame containing the response and predictor variables")])]),t._v(" "),a("tr",[a("td",[t._v("subset")]),t._v(" "),a("td",[t._v("a vector specifying a subset of observations to be used: may be expressed as a logical statement in terms of the variables in "),a("code",[t._v("data")])])]),t._v(" "),a("tr",[a("td",[t._v("weights")]),t._v(" "),a("td",[t._v("analytical weights (see "),a("strong",[t._v("Weights")]),t._v(" section above)")])]),t._v(" "),a("tr",[a("td",[t._v("na.action")]),t._v(" "),a("td",[t._v("how to handle missing ("),a("code",[t._v("NA")]),t._v(") values: see "),a("code",[t._v("?na.action")])])]),t._v(" "),a("tr",[a("td",[t._v("method")]),t._v(" "),a("td",[t._v("how to perform the fitting. Only choices are "),a("code",[t._v('"qr"')]),t._v(" or "),a("code",[t._v('"model.frame"')]),t._v(" (the latter returns the model frame without fitting the model, identical to specifying "),a("code",[t._v("model=TRUE")]),t._v(")")])]),t._v(" "),a("tr",[a("td",[t._v("model")]),t._v(" "),a("td",[t._v("whether to store the model frame in the fitted object")])]),t._v(" "),a("tr",[a("td",[t._v("x")]),t._v(" "),a("td",[t._v("whether to store the model matrix in the fitted object")])]),t._v(" "),a("tr",[a("td",[t._v("y")]),t._v(" "),a("td",[t._v("whether to store the model response in the fitted object")])]),t._v(" "),a("tr",[a("td",[t._v("qr")]),t._v(" "),a("td",[t._v("whether to store the QR decomposition in the fitted object")])]),t._v(" "),a("tr",[a("td",[t._v("singular.ok")]),t._v(" "),a("td",[t._v("whether to allow "),a("strong",[t._v("singular fits")]),t._v(", models with collinear predictors (a subset of the coefficients will automatically be set to "),a("code",[t._v("NA")]),t._v(" in this case")])]),t._v(" "),a("tr",[a("td",[t._v("contrasts")]),t._v(" "),a("td",[t._v("a list of contrasts to be used for particular factors in the model; see the "),a("code",[t._v("contrasts.arg")]),t._v(" argument of "),a("code",[t._v("?model.matrix.default")]),t._v(". Contrasts can also be set with "),a("code",[t._v("options()")]),t._v(" (see the "),a("code",[t._v("contrasts")]),t._v(" argument) or by assigning the "),a("code",[t._v("contrast")]),t._v(" attributes of a factor (see "),a("code",[t._v("?contrasts")]),t._v(")")])]),t._v(" "),a("tr",[a("td",[t._v("offset")]),t._v(" "),a("td",[t._v("used to specify an "),a("strong",[t._v("a priori")]),t._v(" known component in the model. May also be specified as part of the formula. See "),a("code",[t._v("?model.offset")])])]),t._v(" "),a("tr",[a("td",[t._v("...")]),t._v(" "),a("td",[t._v("additional arguments to be passed to lower-level fitting functions ("),a("code",[t._v("lm.fit()")]),t._v(" or "),a("code",[t._v("lm.wfit()")]),t._v(")")])])])])])}),[],!1,null,null,null);s.default=n.exports}}]);