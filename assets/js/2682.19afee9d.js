(window.webpackJsonp=window.webpackJsonp||[]).push([[2682],{3027:function(t,s,a){"use strict";a.r(s);var n=a(19),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"data-serialization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#data-serialization"}},[t._v("#")]),t._v(" Data Serialization")]),t._v(" "),a("h2",{attrs:{id:"serialization-using-json"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#serialization-using-json"}},[t._v("#")]),t._v(" Serialization using JSON")]),t._v(" "),a("p",[a("strong",[t._v("JSON")]),t._v(" is a cross language, widely used method to serialize data")]),t._v(" "),a("p",[t._v("Supported data types : "),a("strong",[t._v("int")]),t._v(", "),a("strong",[t._v("float")]),t._v(", "),a("strong",[t._v("boolean")]),t._v(", "),a("strong",[t._v("string")]),t._v(", "),a("strong",[t._v("list")]),t._v(" and "),a("strong",[t._v("dict")]),t._v(". See -> "),a("a",{attrs:{href:"https://en.wikipedia.org/wiki/JSON",target:"_blank",rel:"noopener noreferrer"}},[t._v("JSON Wiki"),a("OutboundLink")],1),t._v(" for more")]),t._v(" "),a("p",[t._v("Here is an example demonstrating the "),a("strong",[t._v("basic")]),t._v(" usage of "),a("strong",[t._v("JSON")]),t._v(" :-")]),t._v(" "),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" json\n\nfamilies "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'John'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mark'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'David'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'name'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Avraham'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Dumping it into string")]),t._v("\njson_families "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dumps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("families"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# [["John"], ["Mark", "David", {"name": "Avraham"}]]')]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Dumping it to file")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'families.json'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" json_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    json"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dump"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("families"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" json_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Loading it from string")]),t._v("\njson_families "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loads"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("json_families"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Loading it from file")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'families.json'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" json_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    json_families "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("json_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("See "),a("a",{attrs:{href:"http://stackoverflow.com/documentation/python/272/json-module",target:"_blank",rel:"noopener noreferrer"}},[t._v("JSON-Module"),a("OutboundLink")],1),t._v(" for detailed information about JSON.")]),t._v(" "),a("h2",{attrs:{id:"serialization-using-pickle"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#serialization-using-pickle"}},[t._v("#")]),t._v(" Serialization using Pickle")]),t._v(" "),a("p",[t._v("Here is an example demonstrating the "),a("strong",[t._v("basic")]),t._v(" usage of "),a("strong",[t._v("pickle")]),t._v(":-")]),t._v(" "),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Importing pickle")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("try")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" cPickle "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pickle  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Python 2")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("except")]),t._v(" ImportError"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pickle  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Python 3")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Creating Pythonic object:")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Family")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("object")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" names"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sons "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" names\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__str__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sons"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmy_family "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Family"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'John'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'David'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Dumping to string")]),t._v("\npickle_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pickle"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dumps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("my_family"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pickle"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("HIGHEST_PROTOCOL"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Dumping to file")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'family.p'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pickle_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    pickle"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dump"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("families"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pickle_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pickle"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("HIGHEST_PROTOCOL"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Loading from string")]),t._v("\nmy_family "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pickle"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loads"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pickle_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Loading from file")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'family.p'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pickle_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    my_family "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pickle"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pickle_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("See "),a("a",{attrs:{href:"http://stackoverflow.com/documentation/python/2606/serializing-python-objects",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pickle"),a("OutboundLink")],1),t._v(" for detailed information about Pickle.")]),t._v(" "),a("p",[a("strong",[t._v("WARNING")]),t._v(": The official documentation for pickle makes it clear that there are no security guarantees. Don't load any data you don't trust its origin.")]),t._v(" "),a("h4",{attrs:{id:"syntax"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#syntax"}},[t._v("#")]),t._v(" Syntax")]),t._v(" "),a("ul",[a("li",[t._v("unpickled_string = pickle.loads(string)")]),t._v(" "),a("li",[t._v("unpickled_string = pickle.load(file_object)")]),t._v(" "),a("li",[t._v("pickled_string = pickle.dumps([('', 'cmplx'), {('object',): None}], pickle.HIGHEST_PROTOCOL)")]),t._v(" "),a("li",[t._v("pickle.dump(('', 'cmplx'), {('object',): None}], file_object, pickle.HIGHEST_PROTOCOL)")]),t._v(" "),a("li",[t._v("unjsoned_string = json.loads(string)")]),t._v(" "),a("li",[t._v("unjsoned_string = json.load(file_object)")]),t._v(" "),a("li",[t._v("jsoned_string = json.dumps(('a', 'b', 'c', [1, 2, 3]))")]),t._v(" "),a("li",[t._v("json.dump(('a', 'b', 'c', [1, 2, 3]), file_object)")])]),t._v(" "),a("h4",{attrs:{id:"parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",[t._v("Parameter")]),t._v(" "),a("th",[t._v("Details")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[a("code",[t._v("protocol")])]),t._v(" "),a("td",[t._v("Using "),a("code",[t._v("pickle")]),t._v(" or "),a("code",[t._v("cPickle")]),t._v(", it is the method that objects are being Serialized/Unserialized. You probably want to use "),a("code",[t._v("pickle.HIGHEST_PROTOCOL")]),t._v(" here, which means the newest method.")])])])]),t._v(" "),a("h4",{attrs:{id:"remarks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#remarks"}},[t._v("#")]),t._v(" Remarks")]),t._v(" "),a("p",[t._v("Why using JSON?")]),t._v(" "),a("ul",[a("li",[t._v("Cross language support")]),t._v(" "),a("li",[t._v("Human readable")]),t._v(" "),a("li",[t._v("Unlike pickle, it doesn't have the danger of running arbitrary code")])]),t._v(" "),a("p",[t._v("Why not using JSON?")]),t._v(" "),a("ul",[a("li",[t._v("Doesn't support Pythonic data types")]),t._v(" "),a("li",[t._v("Keys in dictionaries must not be other than string data types.")])]),t._v(" "),a("p",[t._v("Why Pickle?")]),t._v(" "),a("ul",[a("li",[t._v("Great way for serializing Pythonic (tuples, functions, classes)")]),t._v(" "),a("li",[t._v("Keys in dictionaries can be of any data type.")])]),t._v(" "),a("p",[t._v("Why not Pickle?")]),t._v(" "),a("ul",[a("li",[t._v("Cross language support is missing")]),t._v(" "),a("li",[t._v("It is not safe for loading arbitrary data")])])])}),[],!1,null,null,null);s.default=e.exports}}]);